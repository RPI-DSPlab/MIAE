{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study the cost of ensemble versus the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# modify this to set up directory:\n",
    "DATA_DIR = \"/home/data/wangz56\"\n",
    "\n",
    "attack_list = [\"losstraj\", \"reference\", \"lira\", \"calibration\"]\n",
    "dataset_list = [\"cifar10\", \"cifar100\", \"cinic10\", \"texas100\", \"purchase100\"]\n",
    "ensemble_method_list = [\"union\", \"intersection\" \"majority_vote\"]\n",
    "seeds = [0, 1, 2, 3, 4, 5]\n",
    "path_to_data = '${DATA_DIR}/miae_experiment_aug_more_target_data'\n",
    "path_to_save_result = '${DATA_DIR}/miae_experiment_aug_more_target_data/ensemble/cost_perf_analysis'\n",
    "if os.path.exists(path_to_save_result) == False:\n",
    "    os.makedirs(path_to_save_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the runtime (cost) of each attack. Attacks are ran on with 2 L40s GPUs in parallel. The GPU's utilization is kept below 100% to avoid any performance degradation. The settings for shadow model and target model are all resnet56\n",
    "\n",
    "breakdown of time for each attack:\n",
    "- losstraj: 17 \n",
    "- reference: 540 (for 20 shadow models) + 8 (for inference on each shadow model) = 548\n",
    "- lira: 540 (for 20 shadow models) + 40 (for augmented queries inference on each shadow model) = 580\n",
    "- calibration: 5\n",
    "\n",
    "The cost of LIRA and reference are so high because they trains 20 shadow models on-line. Meaning that both samples of inference and sample for training are used. Whereas losstraj and calibration only use auxiliary data for preparing attack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cost of each attack in terms of time (minutes)\n",
    "cost_table_time = {\"losstraj\": 17, \n",
    "              \"reference\": 548, \n",
    "              \"lira\": 580, \n",
    "              \"calibration\": 5,\n",
    "              \"lira_shadow_models\": 540\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ensemble_perf(dataset, num_seed, ensemble_method, path_to_data):\n",
    "    path_to_df = f\"{path_to_data}/ensemble/{dataset}/{num_seed}_seeds/{ensemble_method}\"\n",
    "    df = pd.read_pickle(f\"{path_to_df}/ensemble_perf.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert roc and acc of multiple dataset to a csv table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "perf_union_df = pd.DataFrame(columns=[\"losstraj\", \"reference\", \"lira\", \"calibration\", \"dataset\", \"AUC\", \"ACC\", \"cost\", \"num_instance\"]\n",
    "                               ).astype({\"AUC\": float, \"ACC\": float, \"losstraj\": bool, \"reference\": bool, \"lira\": bool, \n",
    "                                         \"calibration\": bool, \"cost\": int, \"num_instance\": int})\n",
    "perf_intersection_df = deepcopy(perf_union_df)\n",
    "perf_mv_df = deepcopy(perf_union_df)\n",
    "\n",
    "for num_seed in range(2, len(seeds)+1):\n",
    "    # merge to a single dataframe\n",
    "    # rows: Ensemble Level, losstraj, reference, lira, calibration, dataset, ensemble_method, auc, acc\n",
    "    for ensemble in ensemble_method_list:\n",
    "        if ensemble == \"majority_vote\":\n",
    "            perf_df = perf_mv_df\n",
    "        elif ensemble == \"union\":\n",
    "            perf_df = perf_union_df\n",
    "        elif ensemble == \"intersection\":\n",
    "            perf_df = perf_intersection_df\n",
    "\n",
    "        for dataset in dataset_list:\n",
    "            df = load_ensemble_perf(dataset, num_seed, ensemble, path_to_data)\n",
    "            for _, row in df.iterrows():\n",
    "                auc = row[\"AUC\"]\n",
    "                acc = row[\"ACC\"]\n",
    "                attack_names = row[\"Attack\"].split(\"_\")\n",
    "                losstraj = \"losstraj\" in attack_names\n",
    "                reference = \"reference\" in attack_names\n",
    "                lira = \"lira\" in attack_names\n",
    "                calibration = \"calibration\" in attack_names\n",
    "\n",
    "                # calculate cost\n",
    "                cost = sum([cost_table_time[attack] for attack in attack_names])\n",
    "                # handle the case of both reference and lira are used\n",
    "                if \"reference\" in attack_names and \"lira\" in attack_names:\n",
    "                    cost -= cost_table_time[\"lira_shadow_models\"]\n",
    "                # account for number of seeds\n",
    "                cost *= num_seed\n",
    "                \n",
    "                new_entry = {\"losstraj\": losstraj, \n",
    "                             \"reference\": reference, \"lira\": lira, \"calibration\": calibration, \n",
    "                             \"dataset\": dataset, \"AUC\": auc, \"ACC\": acc, \"cost\": cost, \"num_instance\": num_seed}\n",
    "                new_entry = pd.DataFrame([new_entry]).astype(perf_df.dtypes.to_dict())\n",
    "                perf_df = pd.concat([perf_df, new_entry], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cost of each attack versus the performance of each attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "\n",
    "if os.path.exists(path_to_save_result) == False:\n",
    "    os.makedirs(path_to_save_result)\n",
    "\n",
    "for ensemble in ensemble_method_list:\n",
    "    if ensemble == \"majority_vote\":\n",
    "        perf_df = perf_mv_df\n",
    "    elif ensemble == \"union\":\n",
    "        perf_df = perf_union_df\n",
    "    elif ensemble == \"intersection\":\n",
    "        perf_df = perf_intersection_df\n",
    "\n",
    "    for dataset in dataset_list:\n",
    "        df = perf_df[perf_df[\"dataset\"] == dataset]\n",
    "\n",
    "        # plot AUC vs cost\n",
    "        plt.figure()\n",
    "        sns.scatterplot(x=\"cost\", y=\"TPR@0.001FPR\", hue=\"num_instance\", data=df)\n",
    "        plt.xlabel(\"Cost (minutes)\")\n",
    "        plt.ylabel(\"TPR@0.1%FPR\")\n",
    "\n",
    "        plt.savefig(f\"{path_to_save_result}/{dataset}_{ensemble}_cost_vs_perf.pdf\",\n",
    "                     bbox_inches='tight', format='pdf')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
