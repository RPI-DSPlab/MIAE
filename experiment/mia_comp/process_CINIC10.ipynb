{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58badea22cd320d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Process CINIC-10 Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54270754",
   "metadata": {},
   "source": [
    "In this notebook, we extract 30,000 images (3,000 for each class) from each of the 10 classes from either CIFAR 10 portion or the ImagineNet portion of the CINIC-10 dataset. This way, we could have trainset and tesetset for same 10 classes, but from different distribution. Resulting a distribution shift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41007c4d765fecd4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# modify this to set up directory:\n",
    "DATA_DIR = \"/home/data/wangz56\"\n",
    "\n",
    "# add ../../.. to the path (MIAE)\n",
    "import sys\n",
    "currdir = os.getcwd()\n",
    "sys.path.append(os.path.join(currdir, '../../..'))\n",
    "from miae.utils.dataset_utils import dataset_split\n",
    "from miae.utils.set_seed import set_seed\n",
    "\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "path_to_CINIC10 = f'{DATA_DIR}/CINIC10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221d9233343e86c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to download and extract CINIC-10 dataset\n",
    "def download_and_extract_cinic10(url, dest_path):\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.makedirs(dest_path)\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(dest_path, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "        print(\"Download complete.\")\n",
    "    print(f\"Extracting {filename}...\")\n",
    "    with tarfile.open(filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=dest_path)\n",
    "    print(\"Extraction complete.\")\n",
    "\n",
    "    # Function to extract a subset of images from a given dataset path\n",
    "def extract_subset(dataset_path, num_images_per_class, output_dir):\n",
    "    # empty output directory\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    classes = os.listdir(dataset_path)\n",
    "    for c in classes:\n",
    "        cls_path = os.path.join(dataset_path, c)\n",
    "        images = os.listdir(cls_path)\n",
    "        selected_images = random.sample(images, num_images_per_class)\n",
    "        class_dir = os.path.join(output_dir, c)\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "        for img in tqdm(selected_images, desc=f\"Extracting {num_images_per_class} images from class {c} to {output_dir}\"):\n",
    "            img_path = os.path.join(cls_path, img)\n",
    "            shutil.copy(img_path, class_dir)\n",
    "\n",
    "# validate number of images in each class\n",
    "def validate_num_images_per_class(dataset_path, num_images_per_class):\n",
    "    classes = os.listdir(dataset_path)\n",
    "    for c in classes:\n",
    "        cls_path = os.path.join(dataset_path, c)\n",
    "        images = os.listdir(cls_path)\n",
    "        if len(images) < num_images_per_class:\n",
    "            raise ValueError(f\"Class {c} has less than {num_images_per_class} images.\")\n",
    "        \n",
    "        if len(images) > num_images_per_class:\n",
    "            print(f\"Class {c} has more than {num_images_per_class} images. Randomly selecting {num_images_per_class} images.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs and paths for CINIC-10 dataset\n",
    "cinic10_url = 'https://datashare.is.ed.ac.uk/bitstream/handle/10283/3192/CINIC-10.tar.gz?sequence=4&isAllowed=y'\n",
    "cinic10_path = path_to_CINIC10\n",
    "\n",
    "# Download and extract CINIC-10 dataset\n",
    "download_and_extract_cinic10(cinic10_url, cinic10_path)\n",
    "\n",
    "\n",
    "# Paths for CINIC-10 CIFAR-10 and ImageNet subsets\n",
    "cinic10_cifar10_train_path = os.path.join(cinic10_path, 'train')\n",
    "cinic10_imagenet_test_path = os.path.join(cinic10_path, 'test')\n",
    "\n",
    "# Extract 30,000 images from CINIC-10 CIFAR-10 train set (3,000 per class)\n",
    "cifar10_train_subset_dir = f'{path_to_CINIC10}/CINIC10_60ksubset/cifar10_train_subset'\n",
    "extract_subset(cinic10_cifar10_train_path, 3000, cifar10_train_subset_dir)\n",
    "validate_num_images_per_class(cifar10_train_subset_dir, 3000)\n",
    "\n",
    "# Extract 30,000 images from CINIC-10 ImageNet test set (3,000 per class)\n",
    "imagenet_test_subset_dir =  f'{path_to_CINIC10}/CINIC10_60ksubset/imagenet_test_subset'\n",
    "extract_subset(cinic10_imagenet_test_path, 3000, imagenet_test_subset_dir)\n",
    "validate_num_images_per_class(imagenet_test_subset_dir, 3000)\n",
    "\n",
    "print(\"Dataset preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of rows and columns for the subplots\n",
    "num_rows = 1\n",
    "num_cols = 10\n",
    "\n",
    "# Create the first plot\n",
    "fig1, axes1 = plt.subplots(num_rows, num_cols, figsize=(20, 4))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes1 = axes1.flatten()\n",
    "\n",
    "train_classes = os.listdir(cifar10_train_subset_dir)\n",
    "\n",
    "# Iterate over the images and display them as subplots\n",
    "for i, c in enumerate(train_classes):\n",
    "    cls_path = os.path.join(cifar10_train_subset_dir, c)\n",
    "    images = os.listdir(cls_path)\n",
    "    image_path = os.path.join(cls_path, images[0])\n",
    "    image = plt.imread(image_path)\n",
    "    axes1[i].imshow(image)\n",
    "    axes1[i].set_title(c)\n",
    "    axes1[i].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Create the second plot\n",
    "fig2, axes2 = plt.subplots(num_rows, num_cols, figsize=(20, 4))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes2 = axes2.flatten()\n",
    "\n",
    "# Iterate over the images and display them as subplots\n",
    "for i, c in enumerate(train_classes):\n",
    "    cls_path = os.path.join(imagenet_test_subset_dir, c)\n",
    "    images = os.listdir(cls_path)\n",
    "    image_path = os.path.join(cls_path, images[1])\n",
    "    image = plt.imread(image_path)\n",
    "    axes2[i].imshow(image)\n",
    "    axes2[i].set_title(c)\n",
    "    axes2[i].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms as T\n",
    "\n",
    "mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "std = [0.24205776, 0.23828046, 0.25874835]\n",
    "regular_transform = T.Compose([T.ToTensor(),\n",
    "                                T.Normalize(mean=mean, std=std)\n",
    "                                ])\n",
    "\n",
    "augmentation_transform = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4), T.transforms.ToTensor(),\n",
    "                                    T.Normalize(mean=mean, std=std)])\n",
    "\n",
    "\n",
    "class CINIC10(data.Dataset):\n",
    "    def __init__(self, image_folder, transform=augmentation_transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.image_folder[idx]\n",
    "        return (self.transform(img), label)\n",
    "\n",
    "dataset_save_dir = f'{DATA_DIR}/miae_experiment_aug_more_target_data/target/cinic10'\n",
    "\n",
    "cifar10_train_subset_dir = f'{path_to_CINIC10}/CINIC10_60ksubset/cifar10_train_subset'\n",
    "imagenet_test_subset_dir = f'{path_to_CINIC10}/CINIC10_60ksubset/imagenet_test_subset'\n",
    "\n",
    "# saving the target_train, target_test, and auxiliary datasets\n",
    "train_set = CINIC10(datasets.ImageFolder(root=cifar10_train_subset_dir), transform=augmentation_transform)\n",
    "test_set = CINIC10(datasets.ImageFolder(root=imagenet_test_subset_dir), transform=augmentation_transform)\n",
    "\n",
    "# build the target_train, target_test, and auxiliary datasets\n",
    "target_train_len, target_test_len = 15000, len(train_set) - 15000\n",
    "print(f\"target_train_len: {target_train_len}, target_test_len: {target_test_len}\")  \n",
    "target_trainset, target_testset = dataset_split(train_set, [target_train_len, target_test_len], shuffle_seed=seed)\n",
    "aux_set = test_set\n",
    "\n",
    "target_membership = np.concatenate([np.ones(len(target_trainset)), np.zeros(len(target_testset))])\n",
    "\n",
    "# save with pickle\n",
    "with open(os.path.join(dataset_save_dir, \"target_trainset.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(target_trainset, f)\n",
    "with open(os.path.join(dataset_save_dir, \"target_testset.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(target_testset, f)\n",
    "with open(os.path.join(dataset_save_dir, \"aux_set.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(aux_set, f)\n",
    "\n",
    "dataset_to_attack = data.ConcatDataset([target_trainset, target_testset])\n",
    "index_to_data = {}\n",
    "for i in range(len(dataset_to_attack)):\n",
    "    index_to_data[i] = dataset_to_attack[i]\n",
    "\n",
    "with open(os.path.join(dataset_save_dir, \"index_to_data.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(index_to_data, f)\n",
    "\n",
    "np.save(os.path.join(dataset_save_dir, \"attack_set_membership.npy\"), target_membership)\n",
    "\n",
    "print(f\"Subset saved successfully to {dataset_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "# Load augmented data from CIFAR-10 train set\n",
    "cifar10_train_augmented = torchvision.datasets.CIFAR10(root=path_to_CINIC10, train=True, transform=augmentation_transform, download=True)\n",
    "\n",
    "# Load augmented data from CINIC-10 test set\n",
    "cinic10_test_augmented = torchvision.datasets.ImageFolder(root=cinic10_imagenet_test_path, transform=augmentation_transform)\n",
    "\n",
    "# Define the number of rows and columns for the subplots\n",
    "num_rows = 2\n",
    "num_cols = 10\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 4))\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the CIFAR-10 train augmented data and plot the images\n",
    "for i in range(num_rows * num_cols):\n",
    "    image, label = cifar10_train_augmented[i]\n",
    "    image = image.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(cifar10_train_augmented.classes[label])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Iterate over the CINIC-10 test augmented data and plot the images\n",
    "for i in range(num_rows * num_cols):\n",
    "    image, label = cinic10_test_augmented[i]\n",
    "    image = image.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    axes[i + num_rows * num_cols].imshow(image)\n",
    "    axes[i + num_rows * num_cols].set_title(cinic10_test_augmented.classes[label])\n",
    "    axes[i + num_rows * num_cols].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303868f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
